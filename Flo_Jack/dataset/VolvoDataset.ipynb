{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:\\Users\\giaco\\OneDrive\\Desktop\\TimeSeries\\Volvo\\train_gen1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolvoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path = \"\", verbose=True):\n",
    "        self.data_path = data_path\n",
    "        self.groups = []\n",
    "        #load df in memory\n",
    "        self.volvo_df = pd.read_csv(self.data_path)\n",
    "        #fit one hot encoder on labels\n",
    "        self.risk_encoder = OneHotEncoder()\n",
    "        self.risk_encoder.fit(self.volvo_df[\"risk_level\"].values.reshape(-1, 1))\n",
    "        #preprocess df   \n",
    "        self.volvo_df = self.__preprocess__(verbose)\n",
    "        self.df_list = self.__group_by_chassis__(verbose)\n",
    "        #get statistics\n",
    "        self.n_groups = len(self.df_list)\n",
    "        self.groups_len = [len(df) for df in self.df_list]\n",
    "\n",
    "    def __preprocess__(self, verbose = False):\n",
    "        \"\"\"\n",
    "        Preprocess the volvo df by removing NaN columns, static columns and correlated features\n",
    "        \"\"\"\n",
    "        assert self.volvo_df is not None\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dropping all NaN column\")\n",
    "        self.volvo_df.dropna(axis = 1, inplace=True)\n",
    "        if verbose:\n",
    "            print(\"Dropping all static columns\")\n",
    "        columns_to_drop = self.volvo_df.loc[:, self.volvo_df.apply(pd.Series.nunique) == 1].columns\n",
    "        columns_to_drop = [ x for x in columns_to_drop if x not in [\"Timesteps\", \"ChassisId_encoded\", \"gen\", \"risk_level\"]]\n",
    "        self.volvo_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        return self.volvo_df\n",
    "        \n",
    "    def __group_by_chassis__(self, verbose = True):\n",
    "        assert self.volvo_df is not None\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Grouping by Chassis id\")\n",
    "        #each chassis has now a df with its multivariate time series\n",
    "        self.df_list = [t[1] for t in self.volvo_df.groupby(\"ChassisId_encoded\")]\n",
    "        return self.df_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            idx (int): idx over df_lists of chassis df\n",
    "\n",
    "        Returns:\n",
    "            tuple: time_series, one_hot labels for each point in time series\n",
    "        \"\"\"\n",
    "        assert idx < len(self)\n",
    "        # retrieve the idx-th group\n",
    "        ts = self.df_list[idx].sort_values(by=[\"Timesteps\"], ascending=True)\n",
    "        # retrieve all usefull infromation from that df\n",
    "        chassis = ts[\"ChassisId_encoded\"].iloc[0]\n",
    "        # generate multivariate timesereies (n_timesteps, 289) 289 atm with simple preprocess\n",
    "        time_series = ts.drop([\"Timesteps\", \"ChassisId_encoded\", \"gen\", \"risk_level\"], axis = 1).values\n",
    "        # point_wise labels\n",
    "        timestep_labels = ts[\"risk_level\"]\n",
    "        return time_series , self.risk_encoder.transform(timestep_labels.values.reshape(-1, 1)).todense()\n",
    "    \n",
    "    @staticmethod\n",
    "    def padding_collate_fn(batch):\n",
    "        data, labels = zip(*batch)\n",
    "        # get shapes\n",
    "        n_features = data[0].shape[1]\n",
    "        n_labels = labels[0].shape[1]\n",
    "        # compute max len\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        # allign data with respect to max sequence len\n",
    "        data_alligned = np.zeros((len(batch), max_len, n_features))\n",
    "        labels_allinged = np.zeros((len(batch), max_len, n_labels))\n",
    "        # 0 where we , FLO is happier this way\n",
    "        mask = np.zeros((len(batch), max_len))\n",
    "        # fill with meaningfull data\n",
    "        for i, d in enumerate(data):\n",
    "            data_alligned[i, :d.shape[0], :] = d\n",
    "            labels_allinged[i, :labels[i].shape[0], :] = labels[i]\n",
    "            # set 1 where meaningfull values\n",
    "            mask[i,:d.shape[0]] = 1\n",
    "        return data_alligned, labels_allinged, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping all NaN column\n",
      "Dropping all static columns\n",
      "Grouping by Chassis id\n"
     ]
    }
   ],
   "source": [
    "dataset = VolvoDataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader( dataset, batch_size = 12, collate_fn = VolvoDataset.padding_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 289), (29, 289), (29, 289), (29, 289), (13, 289), (15, 289), (17, 289), (4, 289), (29, 289), (27, 289), (23, 289), (29, 289)]\n",
      "(12, 29, 289) (12, 29, 3) (12, 29)\n",
      "[-84.44648491   0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.        ]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, labels, mask = batch\n",
    "  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jack_predicting-failure-risk-levels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
