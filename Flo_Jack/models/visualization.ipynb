{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import *\n",
    "import os\n",
    "import tqdm\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from models.tcn import MS_TCN\n",
    "from dataset.VolvoDataset import VolvoDatasetPart1, VolvoDatasetPart2\n",
    "from utils.ContinuityCrossEntropyLoss import ContinuityCrossEntropyLoss\n",
    "from utils.StatsComputer import StatsComputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    seed=13_04_2000,\n",
    "    test_only=False,\n",
    "    load_model=\"\",\n",
    "    batch_size=128,\n",
    "    num_epochs=30,\n",
    "    learning_rate=0.0005,\n",
    "    lr_scheduler_gamma=0.8,\n",
    "    lr_scheduler_step=1,\n",
    "    patience_epochs=7,\n",
    "    disable_cuda=False,\n",
    "    data_path=r\"Flo_Jack\\models\\data\",\n",
    "    train_csv=r\"train_gen1.csv\",\n",
    "    test_csv=r\"public_X_test.csv\",\n",
    "    variants_csv=r\"variants.csv\",\n",
    "    tcnn_weights=r\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "### Get important paths\n",
    "dataset_path = args.data_path \n",
    "variants_path = os.path.join(dataset_path, args.variants_csv)\n",
    "train_data_path = os.path.join(dataset_path, args.train_csv)\n",
    "test_data_path = os.path.join(dataset_path, args.test_csv)\n",
    "weights_path = args.tcnn_weights\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "\n",
    "### Get dataset and model type\n",
    "\n",
    "train_data_path = os.path.join(\"data\", \"train_gen1.csv\")\n",
    "test_data_path = os.path.join(\"data\", \"public_X_test.csv\")\n",
    "variants_path = os.path.join(\"data\", \"variants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7280/7280 [00:00<00:00, 14116.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1177 Validation: 305\n",
      "Train: 962 Validation: 245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3359/3359 [00:00<00:00, 15569.19it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VolvoDatasetPart2(data_path=train_data_path, variants_path=variants_path)\n",
    "processor = train_dataset.get_processor()\n",
    "label_encoder = processor.risk_encoder\n",
    "\n",
    "train_dataset, validation_dataset = train_dataset.split_train_validation()\n",
    "\n",
    "test_dataset = VolvoDatasetPart2(data_path=test_data_path, variants_path=variants_path, test=True)\n",
    "test_dataset.set_processor(processor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medium' 'Medium' 'Medium' 'Medium' 'High' 'High' 'High' 'High' 'High'\n",
      " 'High']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0013, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0026, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0092, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0105, 0.9999],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0119, 0.9999]]),\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low']\n",
      "['Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low']\n",
      "['Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low']\n",
      "['Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low']\n",
      "['Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low' 'Low']\n",
      "['High' 'High' 'High' 'High' 'High' 'High' 'High' 'High' 'High' 'High']\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0013, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0026, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0092, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0105, 0.9999],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.9999, 0.0119, 0.9999]]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 1\n"
     ]
    }
   ],
   "source": [
    "for elem in train_dataset:\n",
    "    value, sstat, label = elem\n",
    "    if label == 1:\n",
    "        print(value, sstat, label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_dataset.get_n_features()\n",
    "num_classes = train_dataset.get_n_classes()\n",
    "\n",
    "#check if preprocess is giving some problems\n",
    "assert train_dataset.get_n_features() == test_dataset.get_n_features()\n",
    "\n",
    "model = MS_TCN(num_stages=0, \n",
    "                    num_input_channels=n_features, \n",
    "                    num_classes=num_classes)\n",
    "\n",
    "### Get device\n",
    "device = torch.device(\n",
    "            \"cuda\" if (torch.cuda.is_available() and not args.disable_cuda) else \"cpu\"\n",
    "        )\n",
    "model.to(device)\n",
    "print(f\"Working on {device}\")\n",
    "\n",
    "# Load weights if necessary\n",
    "if args.load_model != \"\":\n",
    "    if not(args.load_model.endswith(\".pth\") or args.load_model.endswith(\".pt\")):\n",
    "        raise Exception(\"Weights file should end with .pt or .pth\")\n",
    "    model_path = os.join.path(weights_path, args.load_model)\n",
    "    print(f\"Loading Model from {model_path}\")\n",
    "    model.load_state_dict(\n",
    "        torch.load( model_path )\n",
    "    )\n",
    "\n",
    "# Create DataLoader instances for train, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                collate_fn = VolvoDataset.padding_collate_fn,\n",
    "                                shuffle=True,\n",
    "                                num_workers=12) #pin_memory=True #consigliano\n",
    "val_loader = DataLoader(validation_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                collate_fn = VolvoDataset.padding_collate_fn, \n",
    "                                shuffle=True,\n",
    "                                num_workers=12)\n",
    "test_loader =  DataLoader(test_dataset, \n",
    "                                batch_size=args.batch_size,\n",
    "                                collate_fn = VolvoDataset.padding_collate_fn)\n",
    "\n",
    "# Define criterion\n",
    "print('Computing class weights...', end='')\n",
    "y = label_encoder.transform( train_dataset.volvo_df[[\"risk_level\"]].values )\n",
    "y = np.argmax(y, axis=1).flatten()\n",
    "y = np.array(y)[0]\n",
    "\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "weights = weights \n",
    "criterion = ContinuityCrossEntropyLoss(\n",
    "    weights=torch.Tensor(weights).to(device),\n",
    "    num_classes=num_classes\n",
    "    )\n",
    "print('done')\n",
    "print('Class weights = ', weights)\n",
    "# criterion = ContinuityCrossEntropyLoss(weights=torch.Tensor([1,1,1]).to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
