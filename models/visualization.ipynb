{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import *\n",
    "import os\n",
    "import tqdm\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from models.tcn import MS_TCN, SS_TCN\n",
    "from dataset.VolvoDataset import VolvoDatasetPart1, VolvoDatasetPart2\n",
    "from utils.ContinuityCrossEntropyLoss import ContinuityCrossEntropyLoss\n",
    "from utils.StatsComputer import StatsComputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VolvoDatasetPart1(data_path=train_data_path, variants_path=variants_path)\n",
    "processor = train_dataset.get_processor()\n",
    "label_encoder = processor.risk_encoder\n",
    "\n",
    "train_dataset, validation_dataset = train_dataset.split_train_validation()\n",
    "\n",
    "test_dataset = VolvoDatasetPart1(data_path=test_data_path, variants_path=variants_path, test=True)\n",
    "test_dataset.set_processor(processor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load state dict from checkpoint file\n",
    "\n",
    "state_dict = torch.load(\"../../TCN_best.pth\")\n",
    "weights_to_keep = [x for x in state_dict.keys() if \"mlp\" not in x]\n",
    "\n",
    "new_state_dict = {}\n",
    "for key in weights_to_keep:\n",
    "    new_state_dict[key] = state_dict[key]\n",
    "\n",
    "new_state_dict.keys()\n",
    "\n",
    "model_2 = SS_TCN(num_input_channels=train_dataset.get_n_features(), \n",
    "                            num_classes=1, \n",
    "                            is_phase_1=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([0.1, 0.2, 0, 0.4, 0.5])\n",
    "tensor_1 = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "def tensor_to_vector(tensor):\n",
    "    tensor = tensor * 10\n",
    "    tensor = torch.round(tensor)\n",
    "    tensor\n",
    "\n",
    "    # for each number, create a tensor with 10 elements, with (1 - number) being 0 and number being 1\n",
    "    list = []\n",
    "    for n in tensor:\n",
    "        list.append(torch.cat([torch.zeros(int(10 - n)), torch.ones(int(n))]))\n",
    "\n",
    "    return torch.stack(list)\n",
    "\n",
    "tensor = tensor_to_vector(tensor)\n",
    "tensor_1 = tensor_to_vector(tensor_1)\n",
    "\n",
    "# count total accuracy\n",
    "\n",
    "correct = torch.eq(tensor, tensor_1).sum().item()\n",
    "total = tensor.numel()\n",
    "correct / total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    seed=13_04_2000,\n",
    "    test_only=False,\n",
    "    load_model=\"\",\n",
    "    batch_size=256,\n",
    "    num_epochs=30,\n",
    "    learning_rate=0.0005,\n",
    "    lr_scheduler_gamma=0.8,\n",
    "    lr_scheduler_step=1,\n",
    "    patience_epochs=7,\n",
    "    disable_cuda=False,\n",
    "    data_path=r\"/data1/malto/volvo_ecml_2024\",\n",
    "    train_csv=r\"train_gen1.csv\",\n",
    "    test_csv=r\"public_X_test.csv\",\n",
    "    variants_csv=r\"variants.csv\",\n",
    "    tcnn_weights=r\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "### Get important paths\n",
    "dataset_path = args.data_path \n",
    "variants_path = os.path.join(dataset_path, args.variants_csv)\n",
    "train_data_path = os.path.join(dataset_path, args.train_csv)\n",
    "test_data_path = os.path.join(dataset_path, args.test_csv)\n",
    "weights_path = args.tcnn_weights\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "\n",
    "### Get dataset and model type\n",
    "train_data_path = os.path.join(args.data_path, \"train_gen1.csv\")\n",
    "test_data_path = os.path.join(args.data_path, \"public_X_test.csv\")\n",
    "variants_path = os.path.join(args.data_path, \"variants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VolvoDatasetPart1(data_path=train_data_path, variants_path=variants_path)\n",
    "processor = train_dataset.get_processor()\n",
    "label_encoder = processor.risk_encoder\n",
    "\n",
    "train_dataset, validation_dataset = train_dataset.split_train_validation()\n",
    "\n",
    "test_dataset = VolvoDatasetPart1(data_path=test_data_path, variants_path=variants_path, test=True)\n",
    "test_dataset.set_processor(processor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_static, y = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.volvo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_dataset = VolvoDatasetPart2(data_path=train_data_path, variants_path=variants_path)\n",
    "processor = train_dataset.get_processor()\n",
    "label_encoder = processor.risk_encoder\n",
    "\n",
    "train_dataset, validation_dataset = train_dataset.split_train_validation()\n",
    "\n",
    "test_dataset = VolvoDatasetPart2(data_path=test_data_path, variants_path=variants_path, test=True)\n",
    "test_dataset.set_processor(processor) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_dataset.get_n_features()\n",
    "num_classes = train_dataset.get_n_classes()\n",
    "\n",
    "#check if preprocess is giving some problems\n",
    "assert train_dataset.get_n_features() == test_dataset.get_n_features()\n",
    "\n",
    "model = SS_TCN(     num_input_channels=n_features, \n",
    "                    num_classes=num_classes, \n",
    "                    is_phase_1=True)\n",
    "\n",
    "### Get device\n",
    "device = torch.device(\n",
    "            \"cuda\" if (torch.cuda.is_available() and not args.disable_cuda) else \"cpu\"\n",
    "        )\n",
    "model.to(device)\n",
    "print(f\"Working on {device}\")\n",
    "\n",
    "# Load weights if necessary\n",
    "if args.load_model != \"\":\n",
    "    if not(args.load_model.endswith(\".pth\") or args.load_model.endswith(\".pt\")):\n",
    "        raise Exception(\"Weights file should end with .pt or .pth\")\n",
    "    model_path = os.join.path(weights_path, args.load_model)\n",
    "    print(f\"Loading Model from {model_path}\")\n",
    "    model.load_state_dict(\n",
    "        torch.load( model_path )\n",
    "    )\n",
    "\n",
    "# Create DataLoader instances for train, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                shuffle=True,\n",
    "                                num_workers=12) #pin_memory=True #consigliano\n",
    "val_loader = DataLoader(validation_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                shuffle=True,\n",
    "                                num_workers=12)\n",
    "test_loader =  DataLoader(test_dataset, \n",
    "                                batch_size=args.batch_size)\n",
    "\n",
    "# Define criterion\n",
    "print('Computing class weights...', end='')\n",
    "weights = train_dataset.get_weights()\n",
    "criterion = BCELoss(weight=torch.Tensor(weights).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "print('done')\n",
    "print('Class weights = ', weights)\n",
    "# criterion = ContinuityCrossEntropyLoss(weights=torch.Tensor([1,1,1]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Start training ===\")\n",
    "print(f\"Batch size: {args.batch_size}\")\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=args.lr_scheduler_gamma, step_size=args.lr_scheduler_step)\n",
    "softmax = torch.nn.functional.softmax\n",
    "waiting_epochs = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1 = 0\n",
    "num_resets = 0\n",
    "for epoch in range(args.num_epochs):\n",
    "    ### Run epoch\n",
    "    print( \"=\"*25, f\"EPOCH {epoch}\", \"=\"*25)\n",
    "    print(\"Learning rate: \", optimizer.param_groups[0]['lr'])\n",
    "    print(\"=== TRAIN ===\")\n",
    "    model.train()     \n",
    "    pbar = tqdm.tqdm(train_loader)\n",
    "    running_loss = 0\n",
    "    i = 0\n",
    "    for timeseries, variants, labels in pbar:\n",
    "        pbar.set_description(f\"Running loss: {running_loss/(i+1e-5) :.4}\")\n",
    "        timeseries, variants, labels = timeseries.to(device), variants.to(device), labels.to(device)\n",
    "        outputs = model(timeseries, variants)\n",
    "        outputs = softmax(outputs, dim=-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    epoch_loss = running_loss / (i+1e-5)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{args.num_epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"=== VAL ===\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(val_loader)\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        i = 0\n",
    "        stats = StatsComputer()\n",
    "        for timeseries, variants, labels in pbar:\n",
    "            pbar.set_description(f\"Running loss: {running_loss/(i+1e-5) :.4}\")\n",
    "            \n",
    "            timeseries, variants, labels = timeseries.to(device), variants.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(timeseries, variants)\n",
    "            outputs = softmax(outputs, dim=-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # outputs = outputs[-1]\n",
    "            acc = torch.sum(torch.argmax(labels, dim = 1) == torch.argmax(outputs, dim = 1))\n",
    "            \n",
    "            running_acc += acc \n",
    "\n",
    "            stats.append(outputs=torch.argmax(outputs, dim=-1).cpu().tolist(), \n",
    "                        labels=torch.argmax(labels, dim=-1).cpu().tolist())\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        validation_loss = running_loss / i\n",
    "        validation_accuracy = running_acc / i\n",
    "        \n",
    "        validation_f1 = stats.macro_avg_f1()\n",
    "        print(stats)\n",
    "    \n",
    "    print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {validation_loss:.4f} vs Best {best_val_loss:.4f}\")\n",
    "    print(f\"Validation F1: {validation_f1} vs Best {best_val_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "    \n",
    "precision, recall, f1, true_sum = precision_recall_fscore_support(flatten(stats.all_labels), flatten(stats.all_outputs)),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
